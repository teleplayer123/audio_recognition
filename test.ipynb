{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.signal as sps\n",
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "def downsample_waveform(waveform, num_bins):\n",
    "    waveform = np.array(waveform)\n",
    "    original_length = len(waveform)\n",
    "    points_per_bin = original_length // num_bins\n",
    "    downsampled_waveform = np.zeros(num_bins)\n",
    "    for i in range(num_bins):\n",
    "        start_index = i * points_per_bin\n",
    "        end_index = start_index + points_per_bin\n",
    "        downsampled_waveform[i] = waveform[start_index:end_index].mean()\n",
    "    return downsampled_waveform.tolist()\n",
    "\n",
    "def add_white_noise(audio):\n",
    "    #generate noise and the scalar multiplier\n",
    "    noise = tf.random.uniform(shape=tf.shape(audio), minval=-1, maxval=1)\n",
    "    noise_scalar = tf.random.uniform(shape=[1], minval=0, maxval=0.2)\n",
    "    # add them to the original audio\n",
    "    audio_with_noise = audio + (noise * noise_scalar)\n",
    "    # final clip the values to ensure they are still between -1 and 1\n",
    "    audio_with_noise = tf.clip_by_value(audio_with_noise, clip_value_min=-1, clip_value_max=1)\n",
    "    return audio_with_noise\n",
    "\n",
    "def extract_features(audio_file_path, window_size=1024, overlap=0, num_bins=16):\n",
    "    sample_rate, audio_data = wavfile.read(audio_file_path)\n",
    "    resampled_audio = sps.resample(audio_data, sample_rate)\n",
    "    # Add white noise to the audio\n",
    "    augmented_audio = add_white_noise(resampled_audio)\n",
    "    step_size = window_size - overlap\n",
    "    num_windows = (len(augmented_audio) - window_size) // step_size + 1\n",
    "    fft_results = []\n",
    "    for i in range(num_windows):\n",
    "        start_index = i * step_size\n",
    "        end_index = start_index + window_size\n",
    "        windowed_signal = augmented_audio[start_index:end_index]\n",
    "        \n",
    "        fft_result = np.fft.fft(windowed_signal)\n",
    "        fft_result = fft_result[0:int(fft_result.shape[0] / 2)]\n",
    "        fft_magnitude = np.abs(fft_result)\n",
    "        fft_magnitude[0] = 0\n",
    "        fft_magnitude = downsample_waveform(fft_magnitude, num_bins)\n",
    "        fft_results.extend(fft_magnitude)\n",
    "    return np.array(fft_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 112)\n",
      "(30,)\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(os.getcwd(), \"rgb_wavs\", \"rgb\")\n",
    "\n",
    "def load_dataset(data_dir):\n",
    "    waveforms = []\n",
    "    labels = []\n",
    "    for dirname in os.listdir(data_dir):\n",
    "        label_dir = os.path.join(data_dir, dirname)\n",
    "        if not dirname in labels:\n",
    "            labels.append(dirname)\n",
    "        wav_files = [os.path.join(label_dir, fname) for fname in os.listdir(label_dir)]\n",
    "        feature_arr = []\n",
    "        for wav_file in wav_files:\n",
    "            xfeatures = extract_features(wav_file)\n",
    "            feature_arr.append(xfeatures)\n",
    "        waveforms.append(np.array(feature_arr))\n",
    "        del feature_arr\n",
    "    return np.array(waveforms), np.array(labels)\n",
    "\n",
    "def load_data(data_dir, color=\"red\"):\n",
    "    labels = []\n",
    "    feature_arr = []\n",
    "    red = 0\n",
    "    green = 0\n",
    "    blue = 0\n",
    "    red_dir = os.path.join(data_dir, \"red\")\n",
    "    green_dir = os.path.join(data_dir, \"green\")\n",
    "    blue_dir = os.path.join(data_dir, \"blue\")\n",
    "    if color == \"red\":\n",
    "        red = 1\n",
    "    elif color == \"green\":\n",
    "        green = 1\n",
    "    elif color == \"blue\":\n",
    "        blue = 1\n",
    "    red_files = [os.path.join(red_dir, fname) for fname in os.listdir(red_dir)[:10]]\n",
    "    for wav_file in red_files:\n",
    "        xfeatures = extract_features(wav_file)\n",
    "        feature_arr.append(xfeatures)\n",
    "        labels.append(red)\n",
    "    green_files = [os.path.join(green_dir, fname) for fname in os.listdir(green_dir)[:10]]\n",
    "    for wav_file in green_files:\n",
    "        xfeatures = extract_features(wav_file)\n",
    "        feature_arr.append(xfeatures)\n",
    "        labels.append(green)\n",
    "    blue_files = [os.path.join(blue_dir, fname) for fname in os.listdir(blue_dir)[:10]]\n",
    "    for wav_file in blue_files:\n",
    "        xfeatures = extract_features(wav_file)\n",
    "        feature_arr.append(xfeatures)\n",
    "        labels.append(blue)\n",
    "    return np.array(feature_arr), np.array(labels)\n",
    "\n",
    "\n",
    "red_audio_data, red_labels = load_data(data_dir, color=\"red\")\n",
    "green_audio_data, green_labels = load_data(data_dir, color=\"green\")\n",
    "blue_audio_data, blue_labels = load_data(data_dir, color=\"blue\")\n",
    "\n",
    "print(np.shape(red_audio_data))\n",
    "print(np.shape(red_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 112)\n",
      "(24,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(red_audio_data, red_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "def normalize(array):\n",
    "    min_val = array.min()\n",
    "    max_val = array.max()\n",
    "    normalized_array = (array - min_val) / (max_val - min_val)\n",
    "    return normalized_array\n",
    "\n",
    "X_norm_train = np.array([normalize(x) for x in X_train])\n",
    "print(np.shape(X_norm_train))\n",
    "print(np.shape(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 644ms/step - accuracy: 0.6842 - loss: 0.6872 - val_accuracy: 0.6000 - val_loss: 0.6839\n",
      "Epoch 2/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6842 - loss: 0.6600 - val_accuracy: 0.6000 - val_loss: 0.6831\n",
      "Epoch 3/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.6842 - loss: 0.6496 - val_accuracy: 0.6000 - val_loss: 0.6832\n",
      "Epoch 4/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.6842 - loss: 0.6354 - val_accuracy: 0.6000 - val_loss: 0.6900\n",
      "Epoch 5/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6842 - loss: 0.6232 - val_accuracy: 0.6000 - val_loss: 0.7062\n",
      "Epoch 6/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6842 - loss: 0.6165 - val_accuracy: 0.6000 - val_loss: 0.7241\n",
      "Epoch 7/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6842 - loss: 0.6138 - val_accuracy: 0.6000 - val_loss: 0.7334\n",
      "Epoch 8/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6842 - loss: 0.6128 - val_accuracy: 0.6000 - val_loss: 0.7368\n",
      "Epoch 9/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6842 - loss: 0.6070 - val_accuracy: 0.6000 - val_loss: 0.7305\n",
      "Epoch 10/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.6842 - loss: 0.5953 - val_accuracy: 0.6000 - val_loss: 0.7163\n",
      "Epoch 11/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.6842 - loss: 0.5842 - val_accuracy: 0.6000 - val_loss: 0.7116\n",
      "Epoch 12/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6842 - loss: 0.5744 - val_accuracy: 0.6000 - val_loss: 0.7058\n",
      "Epoch 13/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6842 - loss: 0.5657 - val_accuracy: 0.6000 - val_loss: 0.6997\n",
      "Epoch 14/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.6842 - loss: 0.5561 - val_accuracy: 0.6000 - val_loss: 0.6929\n",
      "Epoch 15/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6842 - loss: 0.5459 - val_accuracy: 0.6000 - val_loss: 0.6899\n",
      "Epoch 16/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6842 - loss: 0.5348 - val_accuracy: 0.6000 - val_loss: 0.6903\n",
      "Epoch 17/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6842 - loss: 0.5220 - val_accuracy: 0.6000 - val_loss: 0.6935\n",
      "Epoch 18/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6842 - loss: 0.5079 - val_accuracy: 0.6000 - val_loss: 0.6975\n",
      "Epoch 19/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6842 - loss: 0.4934 - val_accuracy: 0.6000 - val_loss: 0.6949\n",
      "Epoch 20/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.7368 - loss: 0.4776 - val_accuracy: 0.6000 - val_loss: 0.6887\n",
      "Epoch 21/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7895 - loss: 0.4603 - val_accuracy: 0.6000 - val_loss: 0.6754\n",
      "Epoch 22/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7895 - loss: 0.4424 - val_accuracy: 0.6000 - val_loss: 0.6616\n",
      "Epoch 23/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8421 - loss: 0.4255 - val_accuracy: 0.8000 - val_loss: 0.6537\n",
      "Epoch 24/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8421 - loss: 0.4077 - val_accuracy: 0.8000 - val_loss: 0.6512\n",
      "Epoch 25/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8421 - loss: 0.3892 - val_accuracy: 0.8000 - val_loss: 0.6524\n",
      "Epoch 26/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8421 - loss: 0.3703 - val_accuracy: 0.8000 - val_loss: 0.6523\n",
      "Epoch 27/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8421 - loss: 0.3521 - val_accuracy: 0.8000 - val_loss: 0.6463\n",
      "Epoch 28/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8421 - loss: 0.3334 - val_accuracy: 0.8000 - val_loss: 0.6340\n",
      "Epoch 29/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9474 - loss: 0.3142 - val_accuracy: 0.8000 - val_loss: 0.6178\n",
      "Epoch 30/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9474 - loss: 0.2954 - val_accuracy: 0.8000 - val_loss: 0.6058\n",
      "Epoch 31/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9474 - loss: 0.2770 - val_accuracy: 0.8000 - val_loss: 0.6001\n",
      "Epoch 32/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9474 - loss: 0.2586 - val_accuracy: 0.8000 - val_loss: 0.5986\n",
      "Epoch 33/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9474 - loss: 0.2406 - val_accuracy: 0.8000 - val_loss: 0.5932\n",
      "Epoch 34/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9474 - loss: 0.2230 - val_accuracy: 0.8000 - val_loss: 0.5798\n",
      "Epoch 35/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9474 - loss: 0.2056 - val_accuracy: 0.8000 - val_loss: 0.5640\n",
      "Epoch 36/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9474 - loss: 0.1884 - val_accuracy: 0.8000 - val_loss: 0.5505\n",
      "Epoch 37/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9474 - loss: 0.1719 - val_accuracy: 0.8000 - val_loss: 0.5390\n",
      "Epoch 38/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9474 - loss: 0.1560 - val_accuracy: 0.8000 - val_loss: 0.5319\n",
      "Epoch 39/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9474 - loss: 0.1407 - val_accuracy: 0.8000 - val_loss: 0.5208\n",
      "Epoch 40/40\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9474 - loss: 0.1260 - val_accuracy: 0.8000 - val_loss: 0.5063\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x211aecf4110>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=(112,), name=\"input_embedding\"))\n",
    "model.add(tf.keras.layers.Dense(12, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(8, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(X_norm_train, y_train, epochs=40, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_and_biases = {}\n",
    "for i, layer in enumerate(model.layers):\n",
    "    weights, biases = layer.get_weights()\n",
    "    weights_and_biases[f'w{i}'] = weights\n",
    "    weights_and_biases[f'b{i}'] = biases\n",
    "\n",
    "# Save the weights and biases to a file\n",
    "np.savez('model_weights_biases.npz', **weights_and_biases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
